{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Day 4: High Entropy Passphrases](https://adventofcode.com/2017/day/4)\n",
    "\n",
    "## Part A\n",
    "\n",
    "Looks like we're taking a major step down in difficulty today - for this puzzle all we want to do is figure out whether a row has any duplicated words on it (to be exact, given a textfile we want to count the number of lines without a duplicate word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_row_validator(f):\n",
    "    assert(f(\"aa bb cc\") == 0)\n",
    "    assert(f(\"aa bb cc bb\") == 1)\n",
    "    assert(f(\"aaa bb a aa\") == 0)\n",
    "    assert(f(\"red yellow green\\nred red red\\nblue green green\") == 2)\n",
    "\n",
    "def row_has_duplicates(row):\n",
    "    words = row.split()\n",
    "    return len(words) != len(set(words))\n",
    "\n",
    "def row_validator(s):\n",
    "    rows = s.split('\\n')\n",
    "    ct = 0\n",
    "    for row in rows:\n",
    "        if row_has_duplicates(row):\n",
    "            ct += 1\n",
    "    return ct\n",
    "\n",
    "def row_validator_short(s):\n",
    "    return sum([int(row_has_duplicates(row)) for row in s.split('\\n')])\n",
    "    \n",
    "test_row_validator(row_validator)\n",
    "test_row_validator(row_validator_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only trick here is how we check if a row has duplicates. Python's `set` is a collection of unique elements, so by constructing a set from a list we automatically toss out any duplicate elements. Then, if the set and original list have different lengths, we know there must've been at least one duplicates. While very clean and short, this solution is actually slightly inefficient since it doesn't break out early upon finding a duplicate (e.g if the row is \"a a b c d e f g h i\" it'll still construct the entire set even though after seeing the first two a's we know that the row has a duplicate). So we can make a slightly faster version that uses the same idea but with a faster exit condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_has_duplicates_fast(row):\n",
    "    words = row.split()\n",
    "    words_set = set()\n",
    "    for word in words:\n",
    "        if word in words_set:\n",
    "            return True\n",
    "        else:\n",
    "            words_set.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get even fancier though we can use Python's `any` function, which is equivalent to the following:\n",
    "\n",
    "```python\n",
    "def any(iterable):\n",
    "    for element in iterable:\n",
    "        if element:\n",
    "            return True\n",
    "    return False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, `any(list)` essentially checks if anything in the list evaluates to True and handles breaking early for us. With this we can hackily shorten to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_has_duplicates_fast_and_short(row):\n",
    "    words = row.split()\n",
    "    words_set = set()\n",
    "    return any(True if word in words_set else words_set.add(word) for word in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticing here that Python's `set.add` returns `None`, which evalutes to `False` if cast to a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "example_set = set()\n",
    "print(example_set.add(3))\n",
    "print(bool(example_set.add(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the timing differences of these functions with some different types of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing no_dup_row:\n",
      "903 µs ± 15.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.62 ms ± 25 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.94 ms ± 64.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Timing all_dup_row:\n",
      "835 µs ± 15.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "329 µs ± 5.68 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "328 µs ± 23.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Timing one_dup_row:\n",
      "937 µs ± 8.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.42 ms ± 27.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.61 ms ± 21.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Timing many_dups_row:\n",
      "926 µs ± 20.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "357 µs ± 7.77 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "351 µs ± 22.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "no_dup_row = ' '.join([str(i) for i in range(10000)])\n",
    "\n",
    "all_dup_row = 'hello ' * 9999 + 'hello'\n",
    "\n",
    "one_dup_row = list(range(10000))\n",
    "one_dup_row[1] = 0 # Now there are two copies of 0\n",
    "random.shuffle(one_dup_row)\n",
    "one_dup_row = ' '.join([str(i) for i in one_dup_row])\n",
    "\n",
    "many_dups_row = list(range(5000)) + list(range(5000))\n",
    "random.shuffle(many_dups_row)\n",
    "many_dups_row = ' '.join([str(i) for i in many_dups_row])\n",
    "\n",
    "print('Timing no_dup_row:')\n",
    "%timeit row_has_duplicates(no_dup_row)\n",
    "%timeit row_has_duplicates_fast(no_dup_row)\n",
    "%timeit row_has_duplicates_fast_and_short(no_dup_row)\n",
    "print()\n",
    "\n",
    "print('Timing all_dup_row:')\n",
    "%timeit row_has_duplicates(all_dup_row)\n",
    "%timeit row_has_duplicates_fast(all_dup_row)\n",
    "%timeit row_has_duplicates_fast_and_short(all_dup_row)\n",
    "print()\n",
    "\n",
    "print('Timing one_dup_row:')\n",
    "%timeit row_has_duplicates(one_dup_row)\n",
    "%timeit row_has_duplicates_fast(one_dup_row)\n",
    "%timeit row_has_duplicates_fast_and_short(one_dup_row)\n",
    "print()\n",
    "\n",
    "print('Timing many_dups_row:')\n",
    "%timeit row_has_duplicates(many_dups_row)\n",
    "%timeit row_has_duplicates_fast(many_dups_row)\n",
    "%timeit row_has_duplicates_fast_and_short(many_dups_row)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first, the expected part: it looks like our \"fast\" exit-early method is indeed a bit faster in the cases where we have lots of duplicates. It seems that the shorter version is about the same as the longer one (so it looks like the `any` function is pretty similar to the actual `for` loop we were using).\n",
    "\n",
    "The slightly weird part is that on the row without duplicates, the \"slower\" method actually performs significantly better (similarly for the one duplicate row, we assume that the one duplicate is relatively late in the list which is why it took a little over half the time of the no duplicate row). I was a bit confused by this at first because I thought that the faster version was essentially the same as the slower one but with an early exit, but then I realized that during set construction, Python actually probably doesn't have to check for membership at all since the same elements will hash to the same thing and thus safely \"overwrite\" each other. So actually in our \"fast\" method we're doing ~twice the work that just constructing the set is because we have to hash twice (once to check if it's in the set, then again when actually adding). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Part B is almost identical for Part A, except that rather than looking for rows without duplicates, we're looking for rows without anagrams. In other words, now if a row contained 'abc' and 'bca' that would invalidate it since they can be rearranged to match each other. The trick here is just to sort each element in the row before doing what we did before. If two words are anagrams (contain the same leters) then after sorting they'll be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_row_anagram_validator(f):\n",
    "    test_row_validator(f) # Should still satisfy all the Part A conditions\n",
    "    assert(f(\"abc cba def\") == 1)\n",
    "    assert(f(\"aaaa aaa aa a\") == 0)\n",
    "    assert(f(\"racecar carrace\") == 1)\n",
    "\n",
    "def row_has_anagrams(row):\n",
    "    words = row.split()\n",
    "    sorted_words = [ ''.join(sorted(word)) for word in words ]\n",
    "    return len(sorted_words) != len(set(sorted_words))\n",
    "\n",
    "def row_anagram_validator(s):\n",
    "    rows = s.split('\\n')\n",
    "    ct = 0\n",
    "    for row in rows:\n",
    "        if row_has_anagrams(row):\n",
    "            ct += 1\n",
    "    return ct\n",
    "    \n",
    "test_row_anagram_validator(row_anagram_validator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
